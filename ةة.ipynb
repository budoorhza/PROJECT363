{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budoorhza/PROJECT363/blob/main/%D8%A9%D8%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h7rcYlqfVKx"
      },
      "source": [
        "# Finding the MRI brain tumor detection dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw5Uf6_hfVKx"
      },
      "source": [
        "Let's find the dataset in this link: https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VXZ9f30fVKy"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NSmo4uifVKy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import random\n",
        "import cv2\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ1DrvqkfVKy"
      },
      "source": [
        "## Reading the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlDueWf7fVKy"
      },
      "outputs": [],
      "source": [
        "tumor = []\n",
        "healthy = []\n",
        "for f in glob.iglob(\"./data/brain_tumor_dataset/yes/*.jpg\"):\n",
        "    img = cv2.imread(f)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    b, g, r = cv2.split(img)\n",
        "    img = cv2.merge([r,g,b])\n",
        "    tumor.append(img)\n",
        "\n",
        "for f in glob.iglob(\"./data/brain_tumor_dataset/no/*.jpg\"):\n",
        "    img = cv2.imread(f)\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    b, g, r = cv2.split(img)\n",
        "    img = cv2.merge([r,g,b])\n",
        "    healthy.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXz3hA6FfVKy"
      },
      "outputs": [],
      "source": [
        "healthy = np.array(healthy)\n",
        "tumor = np.array(tumor)\n",
        "All = np.concatenate((healthy, tumor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8A205_GfVKy"
      },
      "outputs": [],
      "source": [
        "healthy.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sudsf4COfVKz"
      },
      "outputs": [],
      "source": [
        "tumor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUpyxw-lfVKz"
      },
      "outputs": [],
      "source": [
        "np.random.choice(10, 5, replace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKXrLFwXfVKz"
      },
      "source": [
        "# Visualizing Brain MRI Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bkHrQk6fVKz"
      },
      "outputs": [],
      "source": [
        "def plot_random(healthy, tumor, num=5):\n",
        "    healthy_imgs = healthy[np.random.choice(healthy.shape[0], num, replace=False)]\n",
        "    tumor_imgs = tumor[np.random.choice(tumor.shape[0], num, replace=False)]\n",
        "\n",
        "    plt.figure(figsize=(16,9))\n",
        "    for i in range(num):\n",
        "        plt.subplot(1, num, i+1)\n",
        "        plt.title('healthy')\n",
        "        plt.imshow(healthy_imgs[i])\n",
        "\n",
        "    plt.figure(figsize=(16,9))\n",
        "    for i in range(num):\n",
        "        plt.subplot(1, num, i+1)\n",
        "        plt.title('tumor')\n",
        "        plt.imshow(tumor_imgs[i])\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZDwaisfMfVKz"
      },
      "outputs": [],
      "source": [
        "plot_random(healthy, tumor, num=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1YRFcxifVKz"
      },
      "source": [
        " # Create Torch Dataset Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtVY6jOXfVKz"
      },
      "source": [
        "## What is Pytorch's Abstract Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44aowjTSfVKz"
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    \"\"\"An abstract class representing a Dataset.\n",
        "\n",
        "    All other datasets should subclass it. All subclasses should override\n",
        "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
        "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
        "    \"\"\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return ConcatDataset([self, other])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFA_kwEgfVKz"
      },
      "source": [
        "## Creating MRI cutom dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO9ztoIHfVKz"
      },
      "outputs": [],
      "source": [
        "class MRI(Dataset):\n",
        "    def __init__(self):\n",
        "\n",
        "        tumor = []\n",
        "        healthy = []\n",
        "        # cv2 - It reads in BGR format by default\n",
        "        for f in glob.iglob(\"./data/brain_tumor_dataset/yes/*.jpg\"):\n",
        "            img = cv2.imread(f)\n",
        "            img = cv2.resize(img,(128,128)) # I can add this later in the boot-camp for more adventure\n",
        "            b, g, r = cv2.split(img)\n",
        "            img = cv2.merge([r,g,b])\n",
        "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1])) # otherwise the shape will be (h,w,#channels)\n",
        "            tumor.append(img)\n",
        "\n",
        "        for f in glob.iglob(\"./data/brain_tumor_dataset/no/*.jpg\"):\n",
        "            img = cv2.imread(f)\n",
        "            img = cv2.resize(img,(128,128))\n",
        "            b, g, r = cv2.split(img)\n",
        "            img = cv2.merge([r,g,b])\n",
        "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
        "            healthy.append(img)\n",
        "\n",
        "        # our images\n",
        "        tumor = np.array(tumor,dtype=np.float32)\n",
        "        healthy = np.array(healthy,dtype=np.float32)\n",
        "\n",
        "        # our labels\n",
        "        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)\n",
        "        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)\n",
        "\n",
        "        # Concatenates\n",
        "        self.images = np.concatenate((tumor, healthy), axis=0)\n",
        "        self.labels = np.concatenate((tumor_label, healthy_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        sample = {'image': self.images[index], 'label':self.labels[index]}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def normalize(self):\n",
        "        self.images = self.images/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhJuCHaNfVKz"
      },
      "outputs": [],
      "source": [
        "mri_dataset = MRI()\n",
        "mri_dataset.normalize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMUBEEu4fVKz"
      },
      "source": [
        "# Creating a dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CUc2rx-HfVKz"
      },
      "outputs": [],
      "source": [
        "# One way of iterating\n",
        "names={0:'Heathy', 1:'Tumor'}\n",
        "dataloader = DataLoader(mri_dataset, shuffle=True)\n",
        "for i, sample in enumerate(dataloader):\n",
        "    img = sample['image'].squeeze()\n",
        "    img = img.reshape((img.shape[1], img.shape[2], img.shape[0]))\n",
        "    plt.title(names[sample['label'].item()])\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    if i == 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFFM7nfffVK0"
      },
      "source": [
        "# Create a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL6kYHIifVK0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=5),\n",
        "        nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "        nn.Tanh(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=5))\n",
        "\n",
        "        self.fc_model = nn.Sequential(\n",
        "        nn.Linear(in_features=256, out_features=120),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(in_features=120, out_features=84),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(in_features=84, out_features=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_model(x)\n",
        "        x = F.sigmoid(x)\n",
        "\n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNvdIrWUfVK0"
      },
      "source": [
        "# Some Basics of Training and Evaluation in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPbhXxDXfVK0"
      },
      "source": [
        "## model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD6_w8IMfVK0"
      },
      "source": [
        "- Used particularly for inference **NOTHING to DO with gradients!!!**\n",
        "- changes the forward() behaviour of the module it is called up on eg, it disables dropout and has batch norm use the entire population statistics. This is necessary for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry7Fr9pqfVK0"
      },
      "source": [
        "## model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJOns4ppfVK0"
      },
      "source": [
        "- Brings drop out and batch norm to action (i.e., train mode).\n",
        "- Gradients are computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIanWm7IfVK0"
      },
      "source": [
        "## numpy array vs tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCeG9D7VfVK0"
      },
      "source": [
        "The difference between a NumPy array and a tensor is that the tensors are backed by the accelerator memory like GPU and they are immutable, unlike NumPy arrays. You can never update a tensor but create a new one. If you are into machine learning or going to be into it, A Tensor is a suitable choice if you are going to use GPU. A tensor can reside in accelerator’s memory.\n",
        "\n",
        "- The numpy arrays are the core functionality of the numpy package designed to support faster mathematical operations. Unlike python’s inbuilt list data structure, they can only hold elements of a single data type. Library like pandas which is used for data preprocessing is built around the numpy array. **Pytorch tensors are similar to numpy arrays, but can also be operated on CUDA-capable Nvidia GPU.**\n",
        "- Numpy arrays are mainly used in typical machine learning algorithms (such as k-means or Decision Tree in scikit-learn) whereas pytorch tensors are mainly used in deep learning which requires heavy matrix computation.\n",
        "- Unlike numpy arrays, while creating pytorch tensor, it also accepts two other arguments called the device_type (whether the computation happens on CPU or GPU) and the requires_grad (which is used to compute the derivatives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdCMhbs9fVK0"
      },
      "source": [
        "## torch.tensor vs. torch.cuda.tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdsd1cTdfVK0"
      },
      "source": [
        "he key difference is just that torch.Tensor occupies CPU memory while torch.cuda.Tensor occupies GPU memory. Of course operations on a CPU Tensor are computed with CPU while operations for the GPU / CUDA Tensor are computed on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko7EJFXOfVK0"
      },
      "outputs": [],
      "source": [
        "# device will be 'cuda' if a GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# creating a CPU tensor\n",
        "cpu_tensor = torch.rand(10).to(device)\n",
        "# moving same tensor to GPU\n",
        "gpu_tensor = cpu_tensor.to(device)\n",
        "\n",
        "print(cpu_tensor, cpu_tensor.dtype, type(cpu_tensor), cpu_tensor.type())\n",
        "print(gpu_tensor, gpu_tensor.dtype, type(gpu_tensor), gpu_tensor.type())\n",
        "\n",
        "print(cpu_tensor*gpu_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP1DEGNQfVK0"
      },
      "source": [
        "As the underlying hardware interface is completely different, CPU Tensors are just compatible with CPU Tensor and vice versa GPU Tensors are just compatible to GPU Tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUcByygQfVK0"
      },
      "source": [
        "### In which scenario is torch.cuda.Tensor() necessary?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1PA-RJlfVK0"
      },
      "source": [
        "When you want to use GPU acceleration (which is much faster in most cases) for your program, you need to use torch.cuda.Tensor, but you have to make sure that ALL tensors you are using are CUDA Tensors, mixing is not possible here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVkbtGsNfVK0"
      },
      "source": [
        "### tensor.cpu().detach().numpy(): Convert Pytorch tensor to Numpy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5VCpOVnfVK0"
      },
      "source": [
        "As mentioned before, np.ndarray object does not have this extra \"computational graph\" layer and therefore, when converting a torch.tensor to np.ndarray you must explicitly remove the computational graph of the tensor using the detach() command.\n",
        ".cpu() returns a copy of this object in CPU memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKmIiiL6fVK0"
      },
      "source": [
        "# Evaluate a New-Born Neural Network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-sxiP5nfVK1"
      },
      "outputs": [],
      "source": [
        "mri_dataset = MRI()\n",
        "mri_dataset.normalize()\n",
        "device = torch.device('cuda:0')\n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVbq7Q8tfVK1"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QiChWON5fVK1"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "outputs = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for D in dataloader:\n",
        "        image = D['image'].to(device)\n",
        "        label = D['label'].to(device)\n",
        "\n",
        "        y_hat = model(image)\n",
        "\n",
        "        outputs.append(y_hat.cpu().detach().numpy())\n",
        "        y_true.append(label.cpu().detach().numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzihR1-5fVK1"
      },
      "outputs": [],
      "source": [
        "outputs = np.concatenate( outputs, axis=0 ).squeeze()\n",
        "y_true = np.concatenate( y_true, axis=0 ).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDoW7CXZfVK1"
      },
      "outputs": [],
      "source": [
        "def threshold(scores,threshold=0.50, minimum=0, maximum = 1.0):\n",
        "    x = np.array(list(scores))\n",
        "    x[x >= threshold] = maximum\n",
        "    x[x < threshold] = minimum\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jo1Eu2nwfVK1"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_true, threshold(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "e5FsOKHkfVK1"
      },
      "outputs": [],
      "source": [
        "# a better confusion matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(16,9))\n",
        "cm = confusion_matrix(y_true, threshold(outputs))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt='g', ax=ax, annot_kws={\"size\": 20})\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=20)\n",
        "ax.set_ylabel('True labels', fontsize=20)\n",
        "ax.set_title('Confusion Matrix', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(['Healthy','Tumor'], fontsize=20)\n",
        "ax.yaxis.set_ticklabels(['Tumor','Healthy'], fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "06uHjffvfVK1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(outputs)\n",
        "plt.axvline(x=len(tumor), color='r', linestyle='--')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74_5pLflfVK2"
      },
      "source": [
        "# Train the dumb model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yr-AbxY-fVK2"
      },
      "outputs": [],
      "source": [
        "eta = 0.0001\n",
        "EPOCH = 400\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
        "dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=True)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DtWaBLLufVK2"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, EPOCH):\n",
        "    losses = []\n",
        "    for D in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        data = D['image'].to(device)\n",
        "        label = D['label'].to(device)\n",
        "        y_hat = model(data)\n",
        "        # define loss function\n",
        "        error = nn.BCELoss()\n",
        "        loss = torch.sum(error(y_hat.squeeze(), label))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('Train Epoch: {}\\tLoss: {:.6f}'.format(epoch+1, np.mean(losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zu9WsiDfVK2"
      },
      "source": [
        "# Evaluate a smart model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzA56-JbfVK2"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=False)\n",
        "outputs=[]\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for D in dataloader:\n",
        "        image =  D['image'].to(device)\n",
        "        label = D['label'].to(device)\n",
        "\n",
        "        y_hat = model(image)\n",
        "\n",
        "        outputs.append(y_hat.cpu().detach().numpy())\n",
        "        y_true.append(label.cpu().detach().numpy())\n",
        "\n",
        "outputs = np.concatenate( outputs, axis=0 )\n",
        "y_true = np.concatenate( y_true, axis=0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FqqGehOfVK2"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_true, threshold(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMhotzTUfVK2"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, threshold(outputs))\n",
        "plt.figure(figsize=(16,9))\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.xaxis.set_ticklabels(['Tumor','Healthy'])\n",
        "ax.yaxis.set_ticklabels(['Tumor','Healthy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "b3nkGC_jfVK2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(outputs)\n",
        "plt.axvline(x=len(tumor), color='r', linestyle='--')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvz5wkbbfVK2"
      },
      "source": [
        "# Visualising the Feature Maps of the Convolutional Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9r-_1xtfVK2"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcztPPWwfVK2"
      },
      "outputs": [],
      "source": [
        "no_of_layers = 0\n",
        "conv_layers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz0zHI3nfVK3"
      },
      "outputs": [],
      "source": [
        "model_children = list(model.children())\n",
        "model_children"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2vN1DsVfVK3"
      },
      "outputs": [],
      "source": [
        "for child in model_children:\n",
        "    if type(child) == nn.Sequential:\n",
        "        for layer in child.children():\n",
        "            if type(layer) == nn.Conv2d:\n",
        "                no_of_layers += 1\n",
        "                conv_layers.append(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlqHj2w2fVK3"
      },
      "outputs": [],
      "source": [
        "conv_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F-IWyFNfVK3"
      },
      "outputs": [],
      "source": [
        "img = mri_dataset[100]['image']\n",
        "plt.imshow(img.reshape(128,128,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3SPK4NpfVK3"
      },
      "outputs": [],
      "source": [
        "img = torch.from_numpy(img).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3OwO1BofVK3"
      },
      "outputs": [],
      "source": [
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ktXxIu1fVK3"
      },
      "outputs": [],
      "source": [
        "img = img.unsqueeze(0)\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKn7cJmhfVK3"
      },
      "outputs": [],
      "source": [
        "results = [conv_layers[0](img)]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "outputs = results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db0FBr6XfVK3"
      },
      "outputs": [],
      "source": [
        "for num_layer in range(len(outputs)):\n",
        "    plt.figure(figsize=(50, 10))\n",
        "    layer_viz = outputs[num_layer].squeeze()\n",
        "    print(\"Layer \",num_layer+1)\n",
        "    for i, f in enumerate(layer_viz):\n",
        "        plt.subplot(2, 8, i + 1)\n",
        "        plt.imshow(f.detach().cpu().numpy())\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_hHrC4fVK3"
      },
      "source": [
        "# Are We Over-fitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsR6xnrmfVK3"
      },
      "source": [
        "## Preparing a validation set: We need to change the MRI dataset slightly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9l_ucAfVK3"
      },
      "source": [
        "We will need to make changes to our **MRI dataset class**:\n",
        "\n",
        "- Define a function to divide the data into train and validation sets\n",
        "- Define a variable called **mode** to determine whether we are interested in the training OR validation data\n",
        "- Change __len()__ and __getitem__() functions and conditioned over the variable **mode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztebDmX-fVK3"
      },
      "outputs": [],
      "source": [
        "# Import train/test split function from sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Kh4RysfVK3"
      },
      "outputs": [],
      "source": [
        "class MRI(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Variables to hold the Training data and Validation data\n",
        "        self.X_train, self.y_train, self.X_val, self.y_val = None, None, None, None\n",
        "\n",
        "        # A variable to determine if we are interested in retrieving the training OR the validation data\n",
        "        self.mode = 'train'\n",
        "\n",
        "        tumor = []\n",
        "        healthy = []\n",
        "        # cv2 - It reads in BGR format by default\n",
        "        for f in glob.iglob(\"./data/brain_tumor_dataset/yes/*.jpg\"):\n",
        "            img = cv2.imread(f)\n",
        "            img = cv2.resize(img,(128,128)) # I can add this later in the boot-camp for more adventure\n",
        "            b, g, r = cv2.split(img)\n",
        "            img = cv2.merge([r,g,b])\n",
        "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1])) # otherwise the shape will be (h,w,#channels)\n",
        "            tumor.append(img)\n",
        "\n",
        "        for f in glob.iglob(\"./data/brain_tumor_dataset/no/*.jpg\"):\n",
        "            img = cv2.imread(f)\n",
        "            img = cv2.resize(img,(128,128))\n",
        "            b, g, r = cv2.split(img)\n",
        "            img = cv2.merge([r,g,b])\n",
        "            img = img.reshape((img.shape[2],img.shape[0],img.shape[1]))\n",
        "            healthy.append(img)\n",
        "\n",
        "        # our images\n",
        "        tumor = np.array(tumor,dtype=np.float32)\n",
        "        healthy = np.array(healthy,dtype=np.float32)\n",
        "\n",
        "        # our labels\n",
        "        tumor_label = np.ones(tumor.shape[0], dtype=np.float32)\n",
        "        healthy_label = np.zeros(healthy.shape[0], dtype=np.float32)\n",
        "\n",
        "        # Concatenates\n",
        "        self.images = np.concatenate((tumor, healthy), axis=0)\n",
        "        self.labels = np.concatenate((tumor_label, healthy_label))\n",
        "\n",
        "    # Define a function that would separate the data into Training and Validation sets\n",
        "    def train_val_split(self):\n",
        "        self.X_train, self.X_val, self.y_train, self.y_val = \\\n",
        "        train_test_split(self.images, self.labels, test_size=0.20, random_state=42)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Use self.mode to deetrmine whether train or val data is of interest\n",
        "        if self.mode == 'train':\n",
        "            return self.X_train.shape[0]\n",
        "        elif self.mode == 'val':\n",
        "            return self.X_val.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use self.mode to deetrmine whether train or val data is of interest\n",
        "        if self.mode== 'train':\n",
        "            sample = {'image': self.X_train[idx], 'label': self.y_train[idx]}\n",
        "\n",
        "        elif self.mode== 'val':\n",
        "            sample = {'image': self.X_val[idx], 'label': self.y_val[idx]}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def normalize(self):\n",
        "        self.images = self.images/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0689Am4fVK3"
      },
      "source": [
        "# Are we overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9h9P_snfVK4"
      },
      "outputs": [],
      "source": [
        "mri_dataset = MRI()\n",
        "mri_dataset.normalize()\n",
        "mri_dataset.train_val_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuMj9xJUfVK4"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(mri_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLxcy2i7fVK4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAur9H9KfVK4"
      },
      "outputs": [],
      "source": [
        "eta=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=eta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyXY6KMkfVK4"
      },
      "outputs": [],
      "source": [
        "# keep track of epoch losses\n",
        "epoch_train_loss = []\n",
        "epoch_val_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LG6GSMPfVK4"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1,600):\n",
        "    train_losses = []\n",
        "    # train for the current epoch\n",
        "    model.train()\n",
        "    mri_dataset.mode = 'train'\n",
        "    for D in train_dataloader:\n",
        "        # Train the model\n",
        "        optimizer.zero_grad()\n",
        "        data = D['image'].to(device)\n",
        "        label = D['label'].to(device)\n",
        "\n",
        "        y_hat = model(data)\n",
        "        error = nn.BCELoss()\n",
        "        loss = torch.sum(error(y_hat.squeeze(), label))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    epoch_train_loss.append(np.mean(train_losses))\n",
        "\n",
        "    # validate for the current epoch\n",
        "    val_losses = []\n",
        "    model.eval()\n",
        "\n",
        "    mri_dataset.mode = 'val'\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for D in val_dataloader:\n",
        "            data = D['image'].to(device)\n",
        "            label = D['label'].to(device)\n",
        "            y_hat = model(data)\n",
        "            error = nn.BCELoss()\n",
        "            loss = torch.sum(error(y_hat.squeeze(), label))\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "    epoch_val_loss.append(np.mean(val_losses))\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('Train Epoch: {}\\tTrain Loss: {:.6f}\\tVal Loss: {:.6f}'.format(epoch+1, np.mean(train_losses),np.mean(val_losses)))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPj-FhAyfVK4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.plot(epoch_train_loss, c='b', label='Train loss')\n",
        "plt.plot(epoch_val_loss, c='r', label = 'Validation loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlabel('Epochs', fontsize=20)\n",
        "plt.ylabel('Loss', fontsize=20)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}